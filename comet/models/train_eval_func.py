import os
import time
import psutil, tracemalloc, gc
import time

from pytorch3d.transforms import quaternion_to_matrix
from torch.cuda.amp import autocast
from tqdm import tqdm

from minipytorch3d.cameras import get_world_to_view_transform
from minipytorch3d.transform3d import Transform3d
from train_util import check_ni, record_and_print_cpu_memory_and_usage, process_spark_data
from pytorch3d.implicitron.tools import vis_utils
from pytorch3d.vis.plotly_vis import plot_scene
from metric import camera_to_rel_deg, calculate_auc, camera_to_rel_deg2
from lightglue import SuperPoint, SIFT, ALIKED

import psutil
import tracemalloc
import gc
import torch
from datetime import datetime

#
# class MemoryMonitor:
#     def __init__(self):
#         self.process = psutil.Process(os.getpid())
#         self.initial_memory = self.process.memory_info().rss
#         self.peak_memory = self.initial_memory
#         self.history = []
#         self.start_time = datetime.utcnow()
#
#     def get_memory_usage(self):
#         """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
#         memory_info = self.process.memory_info()
#         virtual_memory = psutil.virtual_memory()
#
#         return {
#             'rss': memory_info.rss,  # ç‰©ç†å†…å­˜ä½¿ç”¨
#             'vms': memory_info.vms,  # è™šæ‹Ÿå†…å­˜ä½¿ç”¨
#             'shared': memory_info.shared,  # å…±äº«å†…å­˜
#             'system_total': virtual_memory.total,  # ç³»ç»Ÿæ€»å†…å­˜
#             'system_available': virtual_memory.available,  # ç³»ç»Ÿå¯ç”¨å†…å­˜
#             'system_percent': virtual_memory.percent  # ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
#         }
#
#     def log(self, step, location):
#         """è®°å½•å†…å­˜ä½¿ç”¨"""
#         current = self.get_memory_usage()
#         self.peak_memory = max(self.peak_memory, current['rss'])
#
#         memory_info = {
#             'step': step,
#             'location': location,
#             'time': datetime.utcnow(),
#             'elapsed_time': (datetime.utcnow() - self.start_time).total_seconds(),
#             'memory': current
#         }
#
#         self.history.append(memory_info)
#
#         # æ‰“å°è¯¦ç»†ä¿¡æ¯
#         print(f"\nğŸ“Š Memory Status at {location} (Step {step}):")
#         print(f"Physical Memory (RSS): {current['rss'] / 1024 ** 3:.2f}GB")
#         print(f"Virtual Memory (VMS): {current['vms'] / 1024 ** 3:.2f}GB")
#         print(f"Shared Memory: {current['shared'] / 1024 ** 3:.2f}GB")
#         print(f"Memory Growth: {(current['rss'] - self.initial_memory) / 1024 ** 3:+.2f}GB")
#         print(f"Peak Memory: {self.peak_memory / 1024 ** 3:.2f}GB")
#         print(f"System Memory Usage: {current['system_percent']}%")
#
#         # æ£€æŸ¥å†…å­˜å¢é•¿
#         if current['rss'] > self.initial_memory * 1.5:  # å¦‚æœå¢é•¿è¶…è¿‡50%
#             print("\nâš ï¸ Warning: Significant memory growth detected!")
#             self.analyze_memory_usage()
#
#     def analyze_memory_usage(self):
#         """åˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µ"""
#         print("\nğŸ” Memory Analysis:")
#
#         # è·å–å¤§å‹å¯¹è±¡
#         large_objects = []
#         for obj in gc.get_objects():
#             try:
#                 size = sys.getsizeof(obj)
#                 if size > 1024 * 1024:  # å¤§äº1MBçš„å¯¹è±¡
#                     large_objects.append((type(obj), size))
#             except:
#                 continue
#
#         # æ‰“å°å¤§å‹å¯¹è±¡ä¿¡æ¯
#         if large_objects:
#             print("\nLarge objects in memory:")
#             for obj_type, size in sorted(large_objects, key=lambda x: x[1], reverse=True)[:10]:
#                 print(f"{obj_type.__name__}: {size / 1024 ** 2:.2f}MB")
#
#         # åˆ†æå†…å­˜å¢é•¿è¶‹åŠ¿
#         if len(self.history) > 1:
#             times = [h['elapsed_time'] for h in self.history]
#             memories = [h['memory']['rss'] for h in self.history]
#
#             if len(times) > 1:
#                 growth_rate = (memories[-1] - memories[0]) / (times[-1] - times[0])
#                 print(f"\nMemory growth rate: {growth_rate / 1024 ** 3:.3f}GB/s")


# ä½¿ç”¨æ–¹å¼
# memory_monitor = MemoryMonitor() retain_graph=True





class QuaternionCameras:
    """
    Stores camera parameters using quaternion for rotation, without converting to rotation matrices.
    Mimics PerspectiveCameras-style interface.
    """

    def __init__(self, R, T, focal_length=1.0, principal_point=None, device="cpu"):
        self.device = device
        self.R = R.to(device)  # (N, 4) w x y z
        self.T = T.to(device)  # (N, 3)

        N = self.R.shape[0]

        # Format focal_length to (N, 2)
        if isinstance(focal_length, (float, int)):
            self.focal_length = torch.full((N, 2), focal_length, device=device)
        elif isinstance(focal_length, torch.Tensor):
            focal_length = focal_length.to(device)
            if focal_length.dim() == 0:
                self.focal_length = focal_length.expand(N, 2)
            elif focal_length.dim() == 1:
                self.focal_length = focal_length.view(-1, 1).expand(-1, 2)
            elif focal_length.dim() == 2:
                self.focal_length = focal_length
            else:
                raise ValueError("focal_length shape not recognized")
        else:
            raise TypeError("focal_length must be float, int or torch.Tensor")

        # Format principal_point to (N, 2)
        if principal_point is None:
            self.principal_point = torch.zeros((N, 2), device=device)
        else:
            # 1. ä½¿ç”¨ as_tensor ä¿ç•™åŸæ•°æ®ç±»å‹/è®¾å¤‡ï¼Œé¿å…ä¸å¿…è¦çš„æ‹·è´
            principal_point = torch.as_tensor(principal_point, dtype=torch.float32, device=device)
            if principal_point.dim() == 1:
                self.principal_point = principal_point.expand(N, 2)
            elif principal_point.dim() == 2:
                self.principal_point = principal_point
            else:
                raise ValueError("principal_point shape not recognized")

    def __repr__(self):
        return (
            f"QuaternionCameras(batch={self.R.shape[0]}, device={self.device})\n"
            f"  q: {self.R.shape}, T: {self.T.shape}\n"
            f"  focal_length: {self.focal_length.shape}, principal_point: {self.principal_point.shape}"
        )

    def get_world_to_view_transform(self) -> Transform3d:
        """
        Converts quaternion rotation and translation to a world-to-view Transform3d.
        """
        R_matrix = quaternion_to_matrix(self.R)  # (N, 3, 3)
        self.R_matrix=R_matrix
        T_vector = self.T  # (N, 3)
        return get_world_to_view_transform(R=R_matrix, T=T_vector)

def train_or_eval_fn(
    model, dataloader, cfg, optimizer, stats, accelerator, lr_scheduler, training=True, epoch=-1
):

    if training: # æ ¹æ® training å‚æ•°ï¼Œåˆ¤æ–­æ˜¯å¦è¿›å…¥è®­ç»ƒæ¨¡å¼ã€‚
        model.train()
    else:
        model.eval() #

    time_start = time.time()
    max_it = len(dataloader) # æ¯ä¸ª epoch ä¸­çš„æ€»æ­¥æ•°ï¼ˆbatch æ•°é‡ï¼‰

    if cfg.track_by_spsg: # è¿™ä¸ªæ¡ä»¶åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ SuperPoint æˆ–å…¶ä»–ç‰¹å¾æå–å™¨æ¥å¤„ç†å…³é”®ç‚¹ã€‚cfg.track_by_spsg æ˜¯ä¸€ä¸ªé…ç½®é¡¹ï¼Œå†³å®šæ˜¯å¦å¯ç”¨è¿™äº›ç‰¹å¾æå–å™¨ã€‚
        # è¿™äº›è¡Œå¯¼å…¥äº†ä¸€äº›ç‰¹å¾æå–å™¨æ¨¡å—
        # â€”â€” ä¸æ¨ç†é˜¶æ®µä¿æŒä¸€è‡´ï¼Œå¼ºåˆ¶è¾“å‡ºå›ºå®šæ•°é‡çš„å…³é”®ç‚¹ â€”â€”
        sp = SuperPoint(
               max_num_keypoints=cfg.train.track_num,
               detection_threshold=0.005).cuda().eval()
        sift = SIFT(max_num_keypoints=cfg.train.track_num).cuda().eval()

    AUC_scene_dict = {} # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸ï¼Œé€šå¸¸ç”¨äºè®°å½•ä¸ AUC ç›¸å…³çš„åœºæ™¯æ•°æ®ã€‚AUCï¼ˆArea Under Curveï¼‰


    for step, batch in enumerate(tqdm(dataloader)): # è¿›å…¥æ•°æ®åŠ è½½å¾ªç¯ï¼Œé€æ­¥å¤„ç† dataloader ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡ï¼ˆbatchï¼‰ã€‚
        # log_mem(step)
        # print(batch["seq_name"])
        # memory_monitor.log(step, "Batch Start")
        if step == 100: #X å½“è¿­ä»£åˆ°ç¬¬ 100 ä¸ªæ‰¹æ¬¡æ—¶ï¼Œè®°å½•å¹¶æ‰“å°å½“å‰çš„ CPU å†…å­˜ä½¿ç”¨æƒ…å†µã€‚è¿™é€šå¸¸ç”¨äºè°ƒè¯•ã€‚
            record_and_print_cpu_memory_and_usage()

        gt_cameras = None # åˆå§‹åŒ– gt_cameras ä¸º Noneï¼Œç”¨äºå­˜å‚¨åœ°é¢çœŸå®å€¼çš„ç›¸æœºä¿¡æ¯ã€‚


        #############å…ˆè¿›è¡Œæ•°æ®å¤„ç†########
        (
            images,
            translation,
            rotation,
            fl,
            pp
        ) = process_spark_data(batch, accelerator.device, cfg)
        # æˆ‘ä»¬å‡è®¾ process_spark_data è¿”å›çš„ images å½¢çŠ¶æ˜¯ [B, S, C, H, W]ï¼›translationã€rotationã€flã€pp åˆ†åˆ«æ˜¯ [B,S,3]ã€[B,S,3,3]ã€[B,S,2]ã€[B,S,2]
        # è°ƒç”¨ process_co3d_data å‡½æ•°æ¥å¤„ç†å½“å‰æ‰¹æ¬¡çš„batchï¼Œæ˜¯ä¸€ä¸ªå­—å…¸ã€‚è¿™ä¸ªå‡½æ•°è¿”å›ä¸€ç»„å›¾åƒå’Œå…¶ä»–ç›¸å…³æ•°æ®ï¼ŒåŒ…æ‹¬å›¾åƒã€å¹³ç§»ã€æ—‹è½¬ã€ç„¦è·ã€ä¸»ç‚¹åæ ‡
        # memory_monitor.log(step, "After Data Processing")


        ######### é€šè¿‡åˆå¹¶ä¸åŒçš„ç‰¹å¾æå–å™¨çš„è¾“å‡ºå¹¶ç­›é€‰å‡ºæ•°é‡æœ€åˆé€‚çš„å…³é”®ç‚¹ï¼Œä½œä¸ºåˆå§‹çš„çš„è½¨è¿¹è·Ÿè¸ªç‚¹ã€‚##########
        if cfg.track_by_spsg and (not cfg.labor_input_traj):
            # åˆ¤æ–­æ˜¯å¦å¯ç”¨äº† track_by_spsgï¼ˆå³å¯ç”¨äº†ç‰¹å¾æå–å™¨ï¼‰ï¼Œå¹¶ä¸”å½“å‰ä¸æ˜¯æ¨ç†é˜¶æ®µï¼ˆcfg.inference ä¸º Falseï¼‰ã€‚
            # use keypoints as the starting points of tracks
            images_for_kp = images[:, 0] # å°†å›¾åƒæ•°æ®èµ‹å€¼ç»™ images_for_kpï¼Œè¿™æ˜¯ç”¨äºæå–å…³é”®ç‚¹çš„æ•°æ®
            bbb,ttt ,_, _, _ = images.shape # bbb æ˜¯æ‰¹æ¬¡å¤§å°ï¼Œnnn æ˜¯è½¨è¿¹æ•°é‡ï¼Œppp æ˜¯æ¯ä¸ªè½¨è¿¹çš„ç»´åº¦ã€‚

            # â€”â€” è°ƒç”¨ extract() æ¥å£ï¼Œä¿è¯è¿”å› [B, track_num, 2] å¼ é‡ â€”â€”
            kp0_sp_list = []
            for i in range(bbb):
                img_i = images_for_kp[i] # [1, C, H, W]
                out_i = sp.extract(img_i, invalid_mask=None)
                kp0_sp_list.append(out_i["keypoints"].squeeze(0))  # [N_sp, 2]

            kp0_sift_list = []
            for i in range(bbb):
                img_i = images_for_kp[i]  # [1, C, H, W]
                out_i = sift.extract(img_i, invalid_mask=None)
                kp0_sift_list.append(out_i["keypoints"].squeeze(0))

            # 1) åˆå¹¶æ¯ä¸ªæ ·æœ¬çš„ SP ä¸ SIFT ç‰¹å¾ç‚¹ï¼Œå¾—åˆ° kp0_list
            kp0_list = [
                torch.cat([sp_pts, sift_pts], dim=0)  # shape=[Ni_sp + Ni_sift, 2]
                for sp_pts, sift_pts in zip(kp0_sp_list, kp0_sift_list)
            ]

            # 2) æ‰¾å‡ºæ‰€æœ‰æ ·æœ¬ä¸­æœ€å°çš„ç‰¹å¾ç‚¹æ•° min_n
            min_n = min(p.shape[0] for p in kp0_list)

            # 3) ç¡®å®šæœ€ç»ˆè¦é€‰çš„ç‚¹æ•° T = min(min_n, cfg.train.track_num)
            T = min(min_n, cfg.train.track_num)

            # 4) å¯¹æ¯ä¸ªæ ·æœ¬éšæœºæŠ½ T ä¸ªç‚¹
            kp0_selected = []
            for p in kp0_list:
                # éšæœºæ‰“ä¹±å¹¶å–å‰ T
                idx = torch.randperm(p.shape[0], device=p.device)[:T]
                kp0_selected.append(p[idx])

            # 5) å †å æˆ [B, T, 2]
            kp0 = torch.stack(kp0_selected, dim=0)  # [B, T, 2]

            new_track_num = kp0.shape[-2] #ä»è·å–åˆå¹¶åçš„å…³é”®ç‚¹æ•°é‡ã€‚
            if new_track_num > cfg.train.track_num: # å¦‚æœå…³é”®ç‚¹æ•°é‡è¶…è¿‡äº†é¢„è®¾çš„æœ€å¤§æ•°é‡ 512ï¼Œåˆ™éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†å…³é”®ç‚¹ã€‚
                indices = torch.randperm(new_track_num)[: cfg.train.track_num] # éšæœºé€‰æ‹©å‰ cfg.train.track_num ä¸ªå…³é”®ç‚¹ã€‚
                kp0 = kp0[:, indices, :] # å°† kp0 é™åˆ¶ä¸ºé€‰æ‹©çš„å…³é”®ç‚¹ã€‚

            # å‡è®¾ kp0 [B, N, 2]
            kp0 = kp0.unsqueeze(1).expand(bbb, ttt, -1, -1)  # [B, ttt, N,2]
            tracks = kp0
            tracks_visibility = torch.ones(bbb, ttt, kp0.shape[-2], device=accelerator.device, dtype=torch.bool)
        else:
            tracks = None
            tracks_visibility = None

        if rotation is not None: # ç›¸æœºå’Œæ•°æ®å¤„ç†ï¼š
            # å¦‚æœ rotationï¼ˆæ—‹è½¬çŸ©é˜µï¼‰ä¸æ˜¯ Noneï¼Œå³æ¨¡å‹çš„æ—‹è½¬çŸ©é˜µå­˜åœ¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†æ ¹æ®æä¾›çš„æ—‹è½¬çŸ©é˜µã€å¹³ç§»å‘é‡ä»¥åŠç„¦è·ã€ä¸»ç‚¹ç­‰ä¿¡æ¯æ¥åˆ›å»ºgtç›¸æœº
            gt_cameras = QuaternionCameras(
                focal_length=fl.reshape(-1, 2), #ç„¦è·
                principal_point=pp.reshape(-1, 2),# ä¸»ç‚¹
                R=rotation.reshape(-1, 4),
                T=translation.reshape(-1, 3),
                device=accelerator.device,
            )

        if training: # è¿›å…¥è®­ç»ƒæ¨¡å¼ã€‚å¦‚æœæ˜¯è®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ã€‚
            predictions = model(
                images,
                gt_cameras=gt_cameras,
                training=True,
                tracks=tracks,
                tracks_visibility=tracks_visibility,
            )
            predictions["loss"] = predictions["loss"].mean()
            loss = predictions["loss"]
        else:
            with torch.no_grad():
                # å¦‚æœè¯„ä¼°æ¨¡å¼ï¼Œåˆ™è°ƒç”¨æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ã€‚ä½¿ç”¨ torch.no_grad() æ¥ç¦æ­¢æ¢¯åº¦è®¡ç®—ï¼Œå› ä¸ºåœ¨è¯„ä¼°æ¨¡å¼ä¸‹æˆ‘ä»¬ä¸éœ€è¦åå‘ä¼ æ’­ã€‚
                predictions = model(
                    images,
                    gt_cameras=gt_cameras,
                    training=False,
                    tracks=tracks,
                    tracks_visibility=tracks_visibility,
                )
            predictions["loss"] = predictions["loss"].mean()

        # memory_monitor.log(step, "After Forward Pass")

        # Computing Metrics ç”¨äº è®¡ç®—è¯„ä¼°æŒ‡æ ‡ è¯„ä¼°é¢„æµ‹çš„ç›¸æœºå§¿æ€ (pred_cameras) ä¸çœŸå®ç›¸æœºå§¿æ€ (gt_cameras) ä¹‹é—´çš„è¯¯å·®ã€‚
        with torch.no_grad(): # ä½¿ç”¨ torch.no_grad() æ¥å‡å°‘å†…å­˜å ç”¨å¹¶åŠ é€Ÿè®¡ç®—ã€‚
            if "pred_cameras" in predictions: # æ£€æŸ¥ predictions å­—å…¸ä¸­æ˜¯å¦åŒ…å« pred_camerasï¼ˆå³æ¨¡å‹æ˜¯å¦é¢„æµ‹äº†ç›¸æœºå‚æ•°ï¼‰
                with autocast(dtype=torch.double):
                    # ä½¿ç”¨ autocast(dtype=torch.double) ä»¥ double (64-bit) ç²¾åº¦è¿›è¡Œè®¡ç®—ï¼Œæé«˜è®¡ç®—ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠæ—‹è½¬çŸ©é˜µå’Œè§’åº¦è®¡ç®—æ—¶
                    ###################éƒ½ä¼ å…¥ç»å¯¹å§¿æ€ è€Œä¸”æ˜¯ç±»çš„å½¢å¼ è¿›è¡Œæ¯”è¾ƒ æ€»å…±286ä¸ªç»“æœ æ˜¯ä¸¤ä¸¤ä¹‹é—´è§†è§’çš„è¯¯å·®ï¼ˆhim and me)###############
                    pred_cameras = predictions["pred_cameras"]  # ç»å¯¹
                    rel_rangle_deg_him, rel_tangle_deg_him = camera_to_rel_deg(
                        pred_cameras, gt_cameras, accelerator.device, bbb
                    )  # è®¡ç®—ç›¸å¯¹æ—‹è½¬è§’è¯¯å·® (rel_rangle_deg) å’Œç›¸å¯¹å¹³ç§»è§’è¯¯å·® å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºè§’åº¦è¯¯å·®ï¼Œä»¥åŠ è®¡ç®—å¹³ç§»å‘é‡çš„è§’åº¦è¯¯å·®
                    ###################éƒ½ä¼ å…¥ç»å¯¹å§¿æ€ è€Œä¸”æ˜¯ç±»çš„å½¢å¼ è¿›è¡Œæ¯”è¾ƒ æ€»å…±286ä¸ªç»“æœ æ˜¯ä¸¤ä¸¤ä¹‹é—´è§†è§’çš„è¯¯å·®###############
                    pred_cameras = predictions["pred_pose_enc"]
                    gt_cameras = predictions["gt_pose_enc"]
                    rel_rangle_deg_rel2one, rel_tangle_deg_rel2one, R_avg, error_euler, _ = camera_to_rel_deg2(
                        pred_cameras, gt_cameras, accelerator.device, bbb
                    )  # è®¡ç®—ç›¸å¯¹æ—‹è½¬è§’è¯¯å·® (rel_rangle_deg) å’Œç›¸å¯¹å¹³ç§»è§’è¯¯å·® å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºè§’åº¦è¯¯å·®ï¼Œä»¥åŠ è®¡ç®—å¹³ç§»å‘é‡çš„è§’åº¦è¯¯å·®
                    ###################éƒ½ä¼ å…¥ç›¸å¯¹å§¿æ€ è€Œä¸”æ˜¯ç±»çš„å½¢å¼ è¿›è¡Œæ¯”è¾ƒ æ€»å…±24ä¸ªç»“æœ æ˜¯ä¸ç¬¬ä¸€å¸§ä¹‹é—´ç›¸å¯¹è§†è§’çš„è¯¯å·®###############

                    predictions["X_err"] = error_euler[2]
                    predictions["Y_err"] = error_euler[1]
                    predictions["Z_err"] = error_euler[0]
                    predictions["R_avg"] = R_avg
                    # metrics to report
                    thresholds = [5, 10, 15]
                    # è®¡ç®—å‡†ç¡®ç‡
                    # 1 æ˜¯ æ¯ä¸¤å¸§ä¹‹é—´éƒ½è®¡ç®—ä¸€æ¬¡è¯¯å·® ä½†æ˜¯ä»–æ˜¯å·¦ä¹˜ æˆ‘å°±è§‰å¾—å’Œæˆ‘ä»¬ä¹‹å‰çš„é‚£ä¸ªä¸ä¸€æ ·
                    # 2 æ˜¯ æ¯ä¸¤å¸§ä¹‹é—´éƒ½è®¡ç®—ä¸€æ¬¡è¯¯å·® å³ä¹˜
                    # 3 æ˜¯ åªå’Œç¬¬ä¸€å¸§ è®¡ç®—ç›¸å¯¹è¯¯å·® å®ƒçš„å¹³ç§» æ˜¯ç›´æ¥ç›¸å‡å¾—åˆ°çš„ ä½†å‰é¢çš„ç›¸å¯¹å¹³ç§»æ˜¯åœ¨ç¬¬ä¸€å¸§çš„åæ ‡ç³»ä¸‹çš„å€¼
                    for threshold in thresholds:
                        predictions[f"Racc_him_{threshold}"] = (
                                rel_rangle_deg_him < threshold).float().mean()  # è®¡ç®— Racc_5, Racc_15, Racc_30ï¼ˆæ—‹è½¬å‡†ç¡®ç‡ï¼‰
                        predictions[f"Tacc_him_{threshold}"] = (
                                rel_tangle_deg_him < threshold).float().mean()  # # ç»“æœ: (1+0+1+0+0)/5 = 0.4
                    # è®¡ç®— AUCï¼ˆç´¯ç§¯è¯¯å·®åˆ†å¸ƒï¼‰  ç´¯è®¡è¯¯å·®æ›²çº¿ (AUC, Area Under Curve)ï¼Œç”¨æ¥è¡¡é‡æ¨¡å‹çš„æ•´ä½“è¡¨ç°ã€‚
                    Auc_30, normalized_histogram = calculate_auc(
                        rel_rangle_deg_him, rel_tangle_deg_him, max_threshold=30, return_list=True
                    )
                    #  è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ AUC ï¼Œåˆ†åˆ«è¡¨ç¤ºè¯¯å·®åœ¨ä¸åŒèŒƒå›´å†…çš„ AUC å€¼ã€‚
                    auc_thresholds = [30, 10, 5, 3]
                    for auc_threshold in auc_thresholds:
                        predictions[f"Auc_{auc_threshold}"] = torch.cumsum(
                            normalized_histogram[:auc_threshold], dim=0
                        ).mean() # è®¡ç®—å‰ auc_threshold ä¸ªè¯¯å·®å€¼çš„ç´¯è®¡å‡å€¼ï¼Œå¾—åˆ° AUC æŒ‡æ ‡ã€‚

                    # åœºæ™¯çº§ AUC
                    scene_name = batch["seq_name"][0]
                    # å…ˆæŠŠå½“å‰åœºæ™¯çš„ AUC è®°å½•åˆ°å­—å…¸ï¼ˆå¯é€‰ï¼‰
                    AUC_scene_dict[scene_name] = torch.cumsum(normalized_histogram[:10], dim=0).mean()
                    # ç„¶åæŠŠå®ƒå†™å› predictions
                    predictions[f"Auc_scene_{scene_name}"] = AUC_scene_dict[scene_name]
                    # batch["seq_name"][0] è¡¨ç¤ºå½“å‰æ‰¹æ¬¡çš„åœºæ™¯åç§°ã€‚
                    # è®¡ç®—è¯¥åœºæ™¯ä¸‹ Auc_10ï¼Œå¹¶å­˜å…¥ AUC_scene_dictï¼Œç”¨äºåç»­åˆ†æä¸åŒåœºæ™¯çš„è¯¯å·®è¡¨ç°ã€‚


        if training:
            stats.update(predictions, time_start=time_start, stat_set="train")
            # ï¼Œè°ƒç”¨ stats.update() æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼Œå…¶ä¸­ä¼ å…¥å½“å‰çš„ predictionsã€å¼€å§‹æ—¶é—´ time_start ä»¥åŠç»Ÿè®¡é›†åˆåç§° stat_set è®¾ç½®ä¸º "train"
            if step % cfg.train.print_interval == 0:
                accelerator.print(stats.get_status_string(stat_set="train", max_it=max_it))
                accelerator.print(
                    f"  Batch Loss Trace:"
                    f"\n    loss_trans   : {predictions.get('loss_trans', 0):.6f}"
                    f"   loss_rot     : {predictions.get('loss_rot', 0):.6f}"
                )
        else:
            stats.update(predictions, time_start=time_start, stat_set="eval")
            if step % cfg.train.eval_print_interval == 0:
                accelerator.print(stats.get_status_string(stat_set="eval", max_it=max_it))
                accelerator.print(
                    f"  Batch Loss Trace:"
                    f"\n    loss_trans   : {predictions.get('loss_trans', 0):.6f}"
                    f"   loss_rot     : {predictions.get('loss_rot', 0):.6f}"
                    f"   loss_motions : {predictions.get('loss_motions', 0):.6f}"
                )

        if training:
            optimizer.zero_grad() #æ¸…ç©ºæ¢¯åº¦
            with torch.autograd.detect_anomaly():
                accelerator.backward(loss)

            # accelerator.backward(loss) # åå‘ä¼ æ’­
            # memory_monitor.log(step, "After Backward Pass")

            # for name, p in model.named_parameters():
            #     if p.grad is None: continue
            #     print(f"{name}: min={p.grad.min():.4e}, max={p.grad.max():.4e}, mean={p.grad.mean():.4e}")
            if cfg.train.clip_grad > 0:
                total_norm_before = accelerator.clip_grad_norm_(model.parameters(), cfg.train.clip_grad)
                # print(f"[GradClip] max_norm={cfg.train.clip_grad}, before={total_norm_before:.4f}")

            # æ¥ä¸‹æ¥æ‰§è¡Œ optimizer.step()ï¼Œåˆ©ç”¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
            # åŒæ—¶ï¼Œè°ƒç”¨ lr_scheduler.step() æ›´æ–°å­¦ä¹ ç‡ï¼Œéµå¾ªé¢„è®¾çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥
            optimizer.step()
            # memory_monitor.log(step, "After Optimizer Step")
            lr_scheduler.step()

            lr_scheduler.step()
            torch.cuda.empty_cache()

            # æ¸…ç†ä¸­é—´å˜é‡
            del predictions
            del images
            del translation
            del rotation
            del fl
            del pp
            del tracks
            del tracks_visibility
            if gt_cameras is not None:
                del gt_cameras
            if training:
                del loss


            # break

    return True